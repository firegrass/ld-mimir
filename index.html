<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Mímir: Semantic Content Pipeline</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" type="text/css" href="style.css">
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
</head>
<body>
<header>

<h1 class="title">Mímir: Semantic Content Pipeline</h1>



</header>
<nav id="TOC">
<ul>
<li><a href="#so-what-is-it">So what is it?</a><ul>
<li><a href="#engineering-versus-office-model-of-document-production">Engineering versus office model of document production</a></li>
<li><a href="#plain-text">Plain-text</a></li>
</ul></li>
<li><a href="#so-what-is-it-1">So what is it?</a></li>
<li><a href="#so-what-is-it-2">So what is it?</a><ul>
<li><a href="#a-web-based-integrated-development-environment">A web based Integrated Development Environment</a></li>
<li><a href="#a-version-control-system">A version control system</a></li>
<li><a href="#a-compiler-service">A compiler / service</a></li>
<li><a href="#a-collaboration-and-project-management-site">A collaboration and project management site</a></li>
<li><a href="#a-continuous-integration-process">A continuous integration process</a></li>
<li><a href="#orchestration-to-manage-development-and-ci-environments-without-admin-overhead">Orchestration to manage development and CI environments without admin overhead</a></li>
</ul></li>
<li><a href="#what-you-should-probably-read-before-working-on-this-project">What you should probably read before working on this project</a></li>
<li><a href="#running-a-mimir-instance">Running a Mimir Instance</a></li>
<li><a href="#running-the-old-linked-data-demo-standalone">Running the old Linked Data Demo Standalone</a></li>
<li><a href="#architecture">Architecture</a><ul>
<li><a href="#principles">Principles</a></li>
<li><a href="#current-technology-selection-in-terms-of-these-principles">Current technology selection in terms of these principles</a></li>
</ul></li>
<li><a href="#what-is-provenance">What is Provenance?</a></li>
<li><a href="#provenance-extraction-from-version-control">Provenance extraction from version control</a></li>
<li><a href="#uri-generation">URI generation</a><ul>
<li><a href="#on-short-hash-length">On short hash length</a></li>
<li><a href="#dereferencability">Dereferencability</a></li>
<li><a href="#resource-uri-generation">Resource URI generation</a></li>
</ul></li>
<li><a href="#knowledgebase-compilation">Knowledgebase compilation</a></li>
<li><a href="#updating-these-docs">Updating these docs</a></li>
<li><a href="#ontology-management">Ontology management</a></li>
<li><a href="#updating-these-docs-1">Updating these docs</a></li>
</ul>
</nav>

<section class="main">
<h2 id="so-what-is-it">So what is it?</h2>
<p>We are building a set of tools that can be used together to create and maintain a semantic knowledge base and derived works.</p>
<h3 id="engineering-versus-office-model-of-document-production">Engineering versus office model of document production</h3>
<p>It is possible to separate approaches to collaboratively constructing documents into two camps<span class="citation" data-cites="Plaintext_Please"><sup>1</sup></span> - the office model and the engineering model. The office model is familiar to most people; a set of tools centered around a word processor and a number of large files are the center of their work. Changes to this work are tracked inside these large files and in various ad-hoc ways. Concurrent editing is difficult unless authors are in the same room and merging changes between different groups working concurrently is usually a manual process. The final output is usually the word processor file without change tracking data, possibly transliterated to formats more suitable for print or web.</p>
<p>In the engineering model, a larger number of plaintext files and a version control system become the center of the project along with a larger suite of more specific tools than a word processor. Changes to these files are tracked externally, in the version control system. Plain text formats are used because of the relative ease of determining the differences between two versions of the same text. Transformation of plain-text is usually performed by different tools than those used for editing it.</p>
<h3 id="plain-text">Plain-text</h3>
<p>Files that contain markup or other data are generally considered plain-text, as long as the entirety remains in directly human-readable form. Markup formats such as HTML and markdown are also plain-text as Coombs, Renear, and DeRose argue<span class="citation" data-cites="Coombs:1987:MSF:32206.32209"><sup>2</sup></span>, punctuation is itself markup. A binary file (such as a JPEG image or a Word Document) is a sequence of bytes which requires interpretation before it can be understood by humans. Some rich text formats such as Microsoft Word’s .xdoc are superficially plain-text but are probably better classed as text encoded binary formats as very few humans would be able to make sense of such a document without the application that renders it.</p>
<p>A consideration when choosing between plain-text formats is the ease of resolving conflicts when multiple agents are collaborating on changes to a single file. Line and character oriented differencing algorithms are efficient and well understood. Differencing rich text formats that have structures that span multiple lines such as HTML or XML is more complex. Using textual differencing on these can produce invalid results and relies on users having knowledge of the format to resolve problems. These same constrains apply to the operational transformation algorithms<span class="citation" data-cites="Sun98operationaltransformation"><sup>3</sup></span> that collaborative real-time editors use.</p>
<h2 id="so-what-is-it-1">So what is it?</h2>
<p>Plain-text files and a modern version control system can be used to create a content repository and publishing pipeline. Version control history can be translated into standard compliant W3C provenance graphs. Semantics can be embedded in plain-text content directly, inferred from structure, extracted using natural language processing or stored as annotations. Print quality documents and web content can be produced from semantic data as the version control system is updated.</p>
<h2 id="so-what-is-it-2">So what is it?</h2>
<p>We are going to try to use similar tools and processes to those we use as developers to produce software to enable NICE to produce semantic content.</p>
<h3 id="a-web-based-integrated-development-environment">A web based Integrated Development Environment</h3>
<figure>
<img src="images/editor.png" alt="Editor" /><figcaption>Editor</figcaption>
</figure>
<h3 id="a-version-control-system">A version control system</h3>
<p>Git.</p>
<h3 id="a-compiler-service">A compiler / service</h3>
<p>Translates plain-text files into an OWL2 knowledge base. This can be syndicated directly and translated into web / pdf / ebook / etc content.</p>
<h3 id="a-collaboration-and-project-management-site">A collaboration and project management site</h3>
<p><a href="http://gitlab.com">Gitlab</a> A github clone we can run behind the firewall.</p>
<h3 id="a-continuous-integration-process">A continuous integration process</h3>
<p>Probably based around <a href="http://drone.io">drone</a></p>
<h3 id="orchestration-to-manage-development-and-ci-environments-without-admin-overhead">Orchestration to manage development and CI environments without admin overhead</h3>
<p>Environments will be executed in <a href="http://docker.io">docker</a>. This makes creation and update of large numbers of instances of the editor and tool chain possible. It also means we need to deploy to Linux, not windows.</p>
<h2 id="what-you-should-probably-read-before-working-on-this-project">What you should probably read before working on this project</h2>
<p>Many of the technologies used are likely to be unfamiliar to people with a background in ASP/.NET. Semantic web technology hasn’t seen much use in the .NET world at all; mostly it’s implemented by specialists using Java.</p>
<h2 id="running-a-mimir-instance">Running a Mimir Instance</h2>
<p>To use Mimir you need to run and link the old linked data demo - this is used as a source of linked data.</p>
<p>To begin, pull the olddemo image.</p>
<pre class="sh"><code>$ docker pull nice/olddemo</code></pre>
<p>Now we need to run it and give it a name. This name is important, so please use ‘olddemo’</p>
<pre class="sh"><code>$ docker run --name olddemo nice/olddemo</code></pre>
<p>Next we pull the Mimir image</p>
<pre class="sh"><code>$ docker pull nice/mimir</code></pre>
<p>Now we need to run mimir. To do this we need to map a directory, forward a port and link to the ‘olddata’ image.</p>
<pre class="sh"><code>$ docker run --link olddemo:olddemo -v ~/my-project-files:/tmp -p 3424:80 --name mimir nice/mimir</code></pre>
<p>This command:</p>
<ul>
<li>forwards port 80 of the docker container to port 3424.</li>
<li>maps the local folder ~/my-project-files to /tmp in the container. This is the folder mimir will use.</li>
<li>creates a link to olddemo. This generates a bunch of environment variables used by mimir.</li>
</ul>
<p>To connect to the Mimir instance, you need the docker host ip address. On Mac OS X, this is:</p>
<pre class="sh"><code>$boot2docker ip</code></pre>
<p>You can then navigate to (in this example’s case) http://${DOCKER_HOST}:3424 and you’re done.</p>
<h2 id="running-the-old-linked-data-demo-standalone">Running the old Linked Data Demo Standalone</h2>
<p>Assuming you have a docker host (boot2docker for example), the process for installing the container:</p>
<pre class="sh"><code>$ docker pull nice/olddemo</code></pre>
<p>And then running:</p>
<pre class="sh"><code>$ docker run nice/olddemo</code></pre>
<p>To find the IP address (the ${DOCKER_HOST}) run:</p>
<pre class="sh"><code>$ boot2docker ip</code></pre>
<p>Then you can navigate to http://${DOCKER_HOST}/index.html</p>
<p>That’s it!</p>
<h2 id="architecture">Architecture</h2>
<h3 id="principles">Principles</h3>
<p>We are building an editor, compiler pipeline and service plus supporting tools for editorial work flow and project management, very similar to something that could produce executable software. The most successful model for this is not the monolithic Visual Studio or Eclipse, but the Makefiles and composable tools used by almost every other platform. This is a classical UNIX architecture, so we can adapt some of its principles to guide us. All design and implementation should comply with the principles outlined below. Any requirement that cannot be implemented without violation of one of more of these principles should be carefully considered.</p>
<p>Consider this a kind of constitution for the development of the system. A net for agile tennis.</p>
<h4 id="isolation">Isolation</h4>
<p>Each editing and compilation environment should be isolated from others.</p>
<h4 id="version-control-and-plain-text-representation">Version control and plain text representation</h4>
<p>All input data should be represented as plain text files in a version control system.</p>
<h4 id="idempotence">Idempotence</h4>
<p>Operations on data should be repeatable and produce the same results.</p>
<h4 id="immutability">Immutability</h4>
<p>Data should be immutable. Building on isolation, version control and idempotence our data can be partitioned by version.</p>
<h4 id="incremental-computation">Incremental computation</h4>
<p>Components should only re-process the minimum possible amount of input data.</p>
<h4 id="ontology-driven">Ontology driven</h4>
<p>Compilation and editing components should not be coupled to any particular ontology, only their outputs.</p>
<h3 id="current-technology-selection-in-terms-of-these-principles">Current technology selection in terms of these principles</h3>
<h4 id="docker">Docker</h4>
<p>We require side by side deployment of different versions of the editor, compiler and their dependencies. Until the last few years or so, the only real option for doing this sort of thing would be to use virtual machines. Vagrant would allow us to produce virtual machine instances as build outputs from team city that we could deploy to both servers and laptops. This however comes with a number of problems. Virtual machine instances are large binaries, so incremental deployment is resource hungry, as is re/booting applications. There are also numerous issues around versioning virtual machines and managing dependencies between them. Enter Docker.</p>
<blockquote>
<p>Docker is an open platform for developers and sysadmins to build, ship, and run distributed applications. Consisting of Docker Engine, a portable, lightweight runtime and packaging tool, and Docker Hub, a cloud service for sharing applications and automating workflows, Docker enables apps to be quickly assembled from components and eliminates the friction between development, QA, and production environments. As a result, IT can ship faster and run the same app, unchanged, on laptops, data center VMs, and any cloud.</p>
</blockquote>
<p>Docker provides similar capabilities to building virtual machines using vagrant, but has a number of advantages:</p>
<ul>
<li>Portable deployment across machines: you can use Docker to create a single object containing all your bundled applications. This object can then be transferred and quickly installed onto any other Docker-enabled host.</li>
<li>Versioning: Docker includes git-like capabilities for tracking successive versions of a container, inspecting the diff between versions, committing new versions, rolling back etc.</li>
<li>Component reuse: Docker allows building or stacking of already created packages. For instance, if you need to create several machines that all require Apache and MySQL database, you can create a ‘base image’ containing these two items, then build and create new machines using these already installed.</li>
<li>Shared libraries: There is already a public registry (http://index.docker.io/ ) where thousands have already uploaded the useful containers they have created. Again, think of the AWS common pool of different configs and distros – this is very similar.</li>
<li>Dependencies: Docker can manage dependencies between containers. So an application container that required redis would discover (and potentially automatically deploy and start) the appropriate services as other containers. This allows us to easily create environments (dev, test, live) etc that will manage all their required dependencies</li>
<li>Incremental deployment: Because of the git-like capabilities deployments use small binary diffs rather than complete images</li>
<li>No boot time: Containers have little or no initialisation / shutdown overhead as they don’t require a complete operating system. This allows fast hot deployments and failover.</li>
</ul>
<p>To use docker containers, our components must run on Linux.</p>
<h4 id="git">Git</h4>
<p>To isolate the version control system it needs to be distributed. Our only practical choices are Git, DARCS, Mercurial or Fossil. Fossil has some interesting features, Mercurial handles binaries well, ARCS has a formal definition and a relatively simple design but Git has been chosen because of the availability of workflow solutions such as Gitlab.</p>
<h2 id="what-is-provenance">What is Provenance?</h2>
<blockquote>
<p>Provenance is information about entities, activities, and people involved in producing a piece of data or thing, which can be used to form assessments about its quality, reliability or trustworthiness<span class="citation" data-cites="McGuinness:13:PTP"><sup>4</sup></span>.</p>
</blockquote>
<p>Provenance is particularly important to the semantic web community as they need to track the complicated web of trust between linked data sources. To meet these needs, the W3C published the PROV standard.</p>
<p>The PROV data model supports the following:</p>
<ul>
<li>The core concepts of identifying an object, attributing the object to person or entity, and representing processing steps</li>
<li>Accessing provenance-related information expressed in other standards</li>
<li>Accessing provenance</li>
<li>The provenance of provenance</li>
<li>Reproducibility</li>
<li>Versioning</li>
<li>Representing procedures</li>
<li>Representing derivation</li>
</ul>
<h2 id="provenance-extraction-from-version-control">Provenance extraction from version control</h2>
<p>The Git2Prov project<span class="citation" data-cites="denies_iswc_2013"><sup>5</sup></span> demonstrates a model for extracting PROV from a git repository. A altered version of this model is presented here that demonstrates the core concepts.</p>
<p>A git repository can be viewed as a sequence of linked commits. Each commit contains the complete repository state at that point and has a unique hash code along with metadata describing the change and identifying the users involved. A diff can be created for any pair of commits that will show altered, added, renamed and copied files.</p>
<p>Our process to translate git into W3C PROV differs slightly from this work (which has a node implementation) and is described here.</p>
<figure>
<img src="images/git.png" alt="Git object structure (c=commit d=diff f=changed files)" /><figcaption>Git object structure (c=commit d=diff f=changed files)</figcaption>
</figure>
<p>To translate from this to the PROV data model we use the following process:</p>
<ul>
<li><p>A start and end commit are selected using their hash or alias</p></li>
<li><p>Commits are topologically sorted</p></li>
<li><p>Commits are grouped into pairs for differencing from the most current to oldest with the oldest commit compared against the empty commit <span class="math"><em>ϵ</em></span>. This produces a sequence of commit pairs:</p></li>
</ul>
<p><br /><span class="math">(<em>c</em><sub><em>n</em></sub>, <em>c</em><sub><em>n</em> − 1</sub>)…(<em>c</em><sub>0</sub>, <em>ϵ</em>)</span><br /></p>
<ul>
<li>A diff is taken between each commit in a pair and associated with the first commit in the pair producing:</li>
</ul>
<p><br /><span class="math">(<em>c</em><sub><em>n</em></sub>, <em>d</em>(<em>c</em><sub><em>n</em></sub>, <em>c</em><sub><em>n</em> − 1</sub>))…(<em>c</em><sub>0</sub>, <em>d</em>(<em>c</em><sub>0</sub>, <em>ϵ</em>))</span><br /></p>
<ul>
<li>Each diff / commit pair is then processed into statements that can be appended to a provenance graph. The commit pairs are then processed into triples as follows:</li>
</ul>
<p>For the commit:</p>
<pre class="ttl"><code>
vcs:commit-{c.SHAHash} a prov:Activity 
  rdfs:label &#39;{c.CommitMessage}&#39; ;
  prov:startedAtTime {c.Author.Time} ;   
  prov:endedAtTime {c.Commit.Time} ;
  prov.wasAssociatedWith vcs:git-user-{c.Commit.User} ;  
  prov.wasInformedBy vcs:commit-{c.Commit.ParentCommit.SHAHash} ;
  prov.qualifiedAssociation 
    [
      a prov:Association ;
      prov:agent vcs:git-user{c.Commit.User} ;
      prov:hadRole &quot;author, comitter&quot; ;
    ]</code></pre>
<p>For each changed file in the commit:</p>
<pre class="ttl"><code>
vcs:file-{f.SHAHash}-{f.FilePath} a prov:Entity,content:ContentAsText ;
  content:chars {git repository http uri}{f.FilePath}-{f.ShaHash} ;
  prov:specializationOf {git repository http uri}{f.FilePath} ;
  prov:wasAttributedTo vcs:git-user-{f.User} ;
  prov:wasGeneratedBy vcs:commit-{c.SHAHash} ;


vcs:commit-{c.SHAHash} prov:uses cs:file-{f.SHAHash}-{f.FilePath} ;
</code></pre>
<h2 id="uri-generation">URI generation</h2>
<p>Handily, git SHA hashes are generated to identify every unique object in git - commits, individual file versions etc. Git also has the ability to generate shortened versions of the long encoded hash strings. We can also use base62 encoding to shorten them further if required.</p>
<h3 id="on-short-hash-length">On short hash length</h3>
<blockquote>
<p>Generally, eight to ten characters are more than enough to be unique within a project. One of the largest Git projects, the Linux kernel, is beginning to need 12 characters out of the possible 40 to stay unique.</p>
</blockquote>
<p>So, initial hash length selection is important. We can come up with a strategy to deal with collisions in the future and git will tell us when our hash length is no longer unique.</p>
<h3 id="dereferencability">Dereferencability</h3>
<p>Git content ids (vcs:file) should be dereferencable, either by representing the content as a property of the uri, or through the use of a service that can resolve them to the text they represent (see github raw URLs for reference). Provenance will be used as input to the resource compiler, so it needs to be able to simply access the content that the provenance graph refers to.</p>
<h3 id="resource-uri-generation">Resource URI generation</h3>
<p>The ‘prov:specialisationOf’ property is a URI that represents the resource at any version, so it is vital that we can compute the same URI with different git histories. So we cannot use a git object hash. We also cannot use an identity generation process (such as guid/uuid tricks) external to git, as this violates our idempotency requirement. So provisionally we will walk the history of the file, find the first commit and take a hash of its path and file name.</p>
<h2 id="knowledgebase-compilation">Knowledgebase compilation</h2>
<p>To compile incrementally compile the knowledge base we need to:</p>
<ul>
<li>Determine what source files have changed since the last compilation</li>
<li>Snapshot those changes so that further changes do not affect the compilation results</li>
<li>Run a configurable set of transformations from plaintext to triples on each source file</li>
<li>Aggregate the newly compiled triples into a graph for insertion</li>
<li>Determine a sparql delete for the previous version of the compiled resources</li>
<li>Store these triples as a build output</li>
</ul>
<p>Provenance derived from git history can be used as input to a compiler, as it can be a complete description of the content changed between 2 points in time.</p>
<p>We extend W3C prov a little to support a compilation entity.</p>
<pre class="omn"><code>
DataProperty: path
    Domain: Target
    Range: string

Class: Compilation
    SubClassOf: prov:Activity

Class: Target
    SubClassOf: prov:Entity
</code></pre>
<p>This lets us supply paths for each changed file, as well as its content.</p>
<h2 id="updating-these-docs">Updating these docs</h2>
<p>While working on a forked version of Mimir:</p>
<ul>
<li>Delete local and remote versions of the <code>gh-pages</code> branch.</li>
<li>Ensure the <code>nhsevidence/ld-mimir</code> repo is added as a remote, called <code>upstream</code> and make sure your local fork is up to date:</li>
</ul>
<pre class="sh"><code>$ git fetch
$ get rebase upstream/master</code></pre>
<ul>
<li>Checkout upstream’s gh-pages branch</li>
</ul>
<pre class="sh"><code>$ git checkout -b gh-pages upstream/gh-pages</code></pre>
<ul>
<li>Go back into the master branch</li>
<li>Commit any changes to the documentation in /docs</li>
<li>with Pandoc and Make installed, run:</li>
</ul>
<pre class="sh"><code>$ make publish # Generates the html versions of the docs, commits and pushes the gh-pages branch</code></pre>
<ul>
<li>Create a pull request from your gh-pages branch to the upstream gh-pages branch.</li>
</ul>
<h2 id="ontology-management">Ontology management</h2>
<p><a href="http://en.wikipedia.org/wiki/Tbox">TBox</a> Ontology statements relating to NICE domains and compilation concerns need to be version controlled and authored as well as the ABox statements generated from content. There are a variety of editors and toolkits appropriate for this, so we will not attempt to build anything. Version control, verification and translation are concerns we need to address.</p>
<p>So applying the plain text principle, we should choose a representation suitable for use with GIT. The <a href="http://www.w3.org/TR/owl2-manchester-syntax/">Manchester OWL syntax</a> is an elegant notation that is supported by the tools we are likely to use.</p>
<p>We will use standard version control practices to maintain the ontologies in the /ns folder of the NICE ontology <a href="https://github.com/nhsevidence/ontologies/tree/master/ns">github</a> repository.</p>
<p>On commit, the input .omn ontologies will be passed into a simple pipeline:</p>
<ul>
<li>Translate .omn to .ttl</li>
<li>Translate .ttl to html documentation</li>
</ul>
<p>Store these pre translated resources in a docker container, along with a simple http service that content negotiates ontology urls:</p>
<ul>
<li>http://ontologies.nice.org.uk/ns/ontologyname With a standard browser accept header -&gt; returns content of ns/ontologyname.html</li>
<li>http://ontologies.nice.org.uk/ns/ontologyname Asking for text/ttl -&gt; returns content of ns/ontologyname.ttl</li>
<li>http://ontologies.nice.org.uk/ns/ontologyname Asking for text/owl-manchester -&gt; returns content of/ontologyname.omn</li>
</ul>
<p><a href="mowl-power.cs.man.ac.uk:8080/converter/">MOWL</a> support translation as a service, but we should no rely on an external dependency with no SLA. So our own translation tools are required.</p>
<h2 id="updating-these-docs-1">Updating these docs</h2>
<p>While working on a forked version of Mimir:</p>
<ul>
<li>Delete local and remote versions of the <code>gh-pages</code> branch.</li>
<li>Ensure the <code>nhsevidence/ld-mimir</code> repo is added as a remote, called <code>upstream</code> and make sure your local fork is up to date:</li>
</ul>
<pre class="sh"><code>$ git fetch
$ get rebase upstream/master</code></pre>
<ul>
<li>Checkout upstream’s gh-pages branch</li>
</ul>
<pre class="sh"><code>$ git checkout -b gh-pages upstream/gh-pages</code></pre>
<ul>
<li>Go back into the master branch</li>
<li>Commit any changes to the documentation in /docs</li>
<li>with Pandoc and Make installed, run:</li>
</ul>
<pre class="sh"><code>$ make publish # Generates the html versions of the docs, commits and pushes the gh-pages branch</code></pre>
<ul>
<li>Create a pull request from your gh-pages branch to the upstream gh-pages branch.</li>
</ul>
<div class="references">
<p>1. Plaintext papers please. Available at: <a href="http://kieranhealy.org/blog/archives/2014/01/23/plain-text/" class="uri">http://kieranhealy.org/blog/archives/2014/01/23/plain-text/</a>.</p>
<p>2. Coombs JH, Renear AH, DeRose SJ. Markup systems and the future of scholarly text processing. <em>Commun ACM</em> 1987;30:933–947. Available at: <a href="http://doi.acm.org/10.1145/32206.32209" class="uri">http://doi.acm.org/10.1145/32206.32209</a>.</p>
<p>3. Sun C, Ellis C (Skip). Operational transformation in real-time group editors: Issues, algorithms, and achievements. 1998.</p>
<p>4. McGuinness D, Lebo T, Sahoo S. PROV-o: The PROV ontology. W3C 2013.</p>
<p>5. De Nies T, Magliacane S, Verborgh R, et al. Git2PROV: Exposing version control system content as W3C PROV. In Poster and demo proceedings of the 12th international semantic web conference. 2013. Available at: <a href="http://www.iswc2013.semanticweb.org/sites/default/files/iswc_demo_32_0.pdf" class="uri">http://www.iswc2013.semanticweb.org/sites/default/files/iswc_demo_32_0.pdf</a>.</p>
</div>
</section>
</body>
</html>